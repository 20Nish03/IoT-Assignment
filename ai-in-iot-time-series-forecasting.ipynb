{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings \n\nfrom sklearn.metrics import mean_squared_error,mean_squared_log_error\nfrom sklearn.model_selection import train_test_split,KFold,StratifiedKFold,GridSearchCV,RandomizedSearchCV,cross_val_score,RepeatedKFold\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer,RobustScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import PolynomialFeatures\n\nimport xgboost as xgb\nimport lightgbm as lgb\nimport sklearn.ensemble as ensemble\nimport sklearn.metrics as metrics\nfrom sklearn.ensemble import RandomForestClassifier,BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier,RandomForestRegressor,BaggingRegressor,AdaBoostRegressor,GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression,LogisticRegression,Lasso, Ridge,LogisticRegressionCV,RidgeCV,LassoCV,ElasticNetCV,OrthogonalMatchingPursuit,ElasticNet,LassoLarsCV,BayesianRidge\nfrom sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC,SVR\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.kernel_ridge import KernelRidge\n\n\nfrom category_encoders.ordinal import OrdinalEncoder\nfrom category_encoders.woe import WOEEncoder\nfrom category_encoders.target_encoder import TargetEncoder\nfrom category_encoders.sum_coding import SumEncoder\nfrom category_encoders.m_estimate import MEstimateEncoder\nfrom category_encoders.leave_one_out import LeaveOneOutEncoder\nfrom category_encoders.helmert import HelmertEncoder\nfrom category_encoders.cat_boost import CatBoostEncoder\nfrom category_encoders.james_stein import JamesSteinEncoder\nfrom category_encoders.one_hot import OneHotEncoder\nfrom scipy.special import boxcox1p\nfrom bayes_opt import BayesianOptimization","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"warnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv('../input/train_1.csv')\ntest=pd.read_csv('../input/test_1.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_mean=train.mean()\ntrain.total_price=train.total_price.fillna(train_mean)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef gen_count_id(train,test,col,name):\n    temp=train.groupby(col)['record_ID'].count().reset_index().rename(columns={'record_ID':name})\n    train=pd.merge(train,temp,how='left',on=col)\n    test=pd.merge(test,temp,how='left',on=col)\n    train[name]=train[name].astype(float)\n    test[name]=test[name].astype(float)\n    train[name].fillna(np.median(temp[name]),inplace=True)\n    test[name].fillna(np.median(temp[name]),inplace=True)\n    return train,test\n\ndef gen_average_units(train,test,col,name):\n    temp=train.groupby(col)['units_sold'].mean().reset_index().rename(columns={'units_sold':name})\n    train=pd.merge(train,temp,how='left',on=col)\n    test=pd.merge(test,temp,how='left',on=col)\n    train[name].fillna(np.median(temp[name]),inplace=True)\n    test[name].fillna(np.median(temp[name]),inplace=True)\n    return train,test\n\ndef gen_average_price(train,test,col,price='base_price',name='name'):\n    temp=train.groupby(col)[price].mean().reset_index().rename(columns={price:name})\n    train=pd.merge(train,temp,how='left',on=col)\n    test=pd.merge(test,temp,how='left',on=col)\n    train[name].fillna(np.median(temp[name]),inplace=True)\n    test[name].fillna(np.median(temp[name]),inplace=True)\n    return train,test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train,test = gen_count_id(train,test,col=['sku_id','store_id'],name='count_id_sku_store') #Genearting count of records per 'sku-id & store-id' \ntrain,test = gen_count_id(train,test,col=['sku_id'],name='count_id_sku') #Genearting count of records per 'sku-id'\ntrain,test = gen_count_id(train,test,col=['store_id'],name='count_id_store') #Genearting count of records per 'store-id'\n\ntrain,test = gen_average_units(train,test,col=['sku_id','store_id'],name='count_sku_store_id') #Genearting average units sold per 'sku-id & store-id'\ntrain,test = gen_average_units(train,test,col=['store_id'],name='count_store_id') #Genearting average units sold per 'store-id'\ntrain,test = gen_average_units(train,test,col=['sku_id'],name='count_sku_id') #Genearting average units sold per 'sku-id'\n\ntrain,test = gen_average_price(train,test,col=['sku_id','store_id'],price='base_price',name='price_sku_store') #Genearting average base price per 'sku-id & store-id'\ntrain,test = gen_average_price(train,test,col=['sku_id','store_id'],price='total_price',name='price_to_sku_store') #Genearting average total price per 'sku-id & store-id'\ntrain,test = gen_average_price(train,test,col=['store_id'],price='base_price',name='price_store_id') #Genearting average base price per 'store-id'\ntrain,test = gen_average_price(train,test,col=['sku_id'],price='base_price',name='price_sku_id') #Genearting average base price per 'sku-id'\ntrain,test = gen_average_price(train,test,col=['store_id'],price='total_price',name='price_to_store_id') #Genearting average total price per 'store-id'\ntrain,test = gen_average_price(train,test,col=['sku_id'],price='total_price',name='price_to_sku_id') #Genearting average total price per 'sku-id'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nle = OrdinalEncoder()\ntrain['week_1']=le.fit_transform(train['week'])\nle = OrdinalEncoder()\ntest['week_1']=le.fit_transform(test['week'])+130\n\n\ntrain['week_num']=train.week_1%52\ntest['week_num']=test.week_1%52\n\ntrain['week_num1']=train.week_1%4\ntest['week_num1']=test.week_1%4\n\n\ntrain['week_sin'] = np.sin(2 * np.pi * train['week_1'] / 52.143)\ntrain['week_cos'] = np.cos(2 * np.pi * train['week_1'] / 52.143)\ntest['week_sin'] = np.sin(2 * np.pi * test['week_1'] / 52.143)\ntest['week_cos'] = np.cos(2 * np.pi * test['week_1'] / 52.143)\n\n\ntrain['price_diff_percent'] = (train['base_price'] - train['total_price']) / train['base_price']\ntest['price_diff_percent'] = (test['base_price'] - test['total_price']) / test['base_price']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=train[list(set(train.columns)-set(['record_ID','units_sold','week']))]\nY= np.log1p(train['units_sold'])\nX_test=test[list(set(test.columns)-set(['record_ID','week']))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X['sku_id'] = X['sku_id'].astype('category')\nX['store_id'] = X['store_id'].astype('category')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"category_list=['store_id','sku_id']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder_final=MEstimateEncoder()\nencoder_final.fit(X[category_list], Y)\n\ncat_enc = encoder_final.transform(X[category_list], Y)\ncontinuous_train = X.drop(columns= category_list)\nX = pd.concat([cat_enc,continuous_train],axis=1)\n\ntest_enc=encoder_final.transform(X_test[category_list])\ncontinuous_test=X_test.drop(columns= category_list)\nX_test=pd.concat([test_enc,continuous_test],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del X['week_num1']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_valid, y_train, y_valid = train_test_split(X, Y, test_size = 0.2,random_state=23)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_base = RandomForestRegressor()\nrf_base.fit(x_train,y_train)\n\n\nrf_tuned = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=30,\n                      max_features='sqrt', max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=1, min_samples_split=10,\n                      min_weight_fraction_leaf=0.0, n_estimators=600,\n                      n_jobs=None, oob_score=True, random_state=None,\n                      verbose=0, warm_start=False)\nrf_tuned.fit(x_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_lgb_base=lgb.LGBMRegressor(objective='regression')\nmodel_lgb_base.fit(x_train,y_train)\n\nmodel_lgb_tuned=lgb.LGBMRegressor(max_depth=2)\n\nmodel_lgb_tuned.fit(x_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_rfb_valid=rf_base.predict(x_valid)\nprediction_rft_valid=rf_tuned.predict(x_valid)\nprediction_lgbmb_valid=model_lgb_base.predict(x_valid)\nprediction_lgbmt_valid=model_lgb_tuned.predict(x_valid)\n\nrf_base_msle=100*mean_squared_log_error(y_valid,prediction_rfb_valid)\nrf_tuned_msle=100*mean_squared_log_error(y_valid,prediction_rft_valid)\nlgbm_base_msle=100*mean_squared_log_error(y_valid,prediction_lgbmb_valid)\nlgbm_tuned_msle=100*mean_squared_log_error(y_valid,prediction_lgbmt_valid)\n\nprediction_ensemble_base=(((1-rf_base_msle)*prediction_rfb_valid)+((1-lgbm_base_msle)*prediction_lgbmb_valid))/(2-rf_base_msle-lgbm_base_msle)\nprediction_ensemble_tuned=(((1-rf_tuned_msle)*prediction_rft_valid)+((1-lgbm_tuned_msle)*prediction_lgbmt_valid))/(2-rf_tuned_msle-lgbm_tuned_msle)\n\nensemble_base_msle=100*mean_squared_log_error(y_valid,prediction_ensemble_base)\nensemble_tuned_msle=100*mean_squared_log_error(y_valid,prediction_ensemble_tuned)\n\n\nprint(\"RF Base: {}; RF Tuned: {}\".format(rf_base_msle,rf_tuned_msle))\nprint(\"LGBM Base: {}; LGBM Tuned: {}\".format(lgbm_base_msle,lgbm_tuned_msle))\nprint(\"Ensemble Base: {}; Ensemble Tuned: {}\".format(ensemble_base_msle,ensemble_tuned_msle))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = lgb.LGBMRegressor(max_depth=2)\n\nmodel.fit(X,Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del X_test['week_num1']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction=model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}